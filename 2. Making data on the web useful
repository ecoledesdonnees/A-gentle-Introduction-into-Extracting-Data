Introduction
Souvent, des données ne sont pas accessibles, alors qu'elles existent. Bien que l’on souhaite pouvoir tout trouver en CSV ou dans un autre format de notre choix, la majorité des données est publiée sous différentes formes sur le web. Et qu'en est-il si vous voulez combiner ces données avec d’autres jeux de données et les explorer indépendamment ?

Le scraping à la rescousse !

Le scraping décrit une méthode pour extraire les données cachées dans des documents, tels que des pages web ou des PDFs, et les rend utilisables pour d’autres traitements. C’est une des compétences les plus utiles si vous avez prévu de faire de l’analyse de données, et la majorité du temps, ce n’est même pas compliqué. Pour utiliser les méthodes de scraping les plus simples, vous n’avez même pas besoin de savoir coder.

La première partie de cet exemple se base principalement sur Google Chrome. Certaines parties du procédé fonctionnent bien dans d’autres navigateurs, cependant nous utiliserons une extension spécifique à Chrome. Si vous ne vous pouvez pas installer Chrome, ne vous inquiétez pas, les principes restent semblables.

Scraping sans une ligne de code, en cinq minutes, en utilisant Google Spreadsheets & Google Chrome
Connaître la structure d’un site web est la première étape vers l’extraction et la réutilisation des données. Allons mettre ces données dans un tableur, afin de pouvoir l’utiliser de façon plus libre. Une façon simple de le faire est fournie par une formule spéciale de Google Spreadsheets.

Épargnez-vous des heures d’agonie passées à copier/coller, à l’aide de la commande ImportHTML de Google Spreadsheets. C’est vraiment magique !

Recettes
Afin de compléter le prochain challenge, regardons dans le Handbook l’une des recettes suivantes :

1. Extraire des données de tableaux HTML
2. Scraping à l’aide de l’extension Scraper de Google Chrome
http://farm9.staticflickr.com/8303/7850933084_b188c02992_o_d.jpg

Ces deux méthodes sont utiles pour :

- Extraire des listes individuelles ou des tableaux issues de pages web uniques
La seconde recette peut remplir des tâches un peu plus complexes, comme extraire de l’information imbriquée. Jetez un œil à la recette pour plus de détails.

Aucune des deux ne fonctionnera pour :

- Extraire des données dispersées sur plusieurs pages web

Challenge
Tâche : Trouvez un site web avec une table et scrapez les informations qui s’y trouve. Partagez vos résultats sur datahub.io (n’oubliez pas de tagger votre jeu de données avec ecoledesdonnees.org).

Astuce
Une fois que vous avez votre table dans un tableur, vous pouvez souhaiter la déplacer, ou la mettre dans une autre feuille de calcul. Pour cela, effectuez un clic droit sur la cellule en haut à gauche, et sélectionnez "paste special" - "paste values only".

Scraping more than one webpage: Scraperwiki
Note: Before proceeding into full scraping mode, it’s helpful to understand the flesh and bones of what makes up a webpage. Read the Introduction to HTML recipe in the handbook.

Until now we’ve only scraped data from a single webpage. What if there are more? Or you want to scrape complex databases? You’ll need to learn how to program – at least a bit.

It’s beyond the scope of this course to teach how to scrape, our aim here is to help you understand whether it is worth investing your time to learn, and to point you at some useful resources to help you on your way!

Structure of a scraper
Scrapers are comprised of three core parts:

1. A queue of pages to scrape
2. An area for structured data to be stored, such as a database
3. A downloader and parser that adds URLs to the queue and/or structured information to the database.
Fortunately for you there is a good website for programming scrapers: ScraperWiki.com

http://farm9.staticflickr.com/8112/8660176200_2dd5aa8d0b_z.jpg

ScraperWiki has two main functions: You can write scrapers – which are optionally run regularly and the data is available to everyone visiting – or you can request them to write scrapers for you. The latter costs some money – however it helps to contact the Scraperwiki community (Google Group) someone might get excited about your project and help you!.

If you are interested in writing scrapers with Scraperwiki, check out this sample scraper – scraping some data about Parliament. Click View source to see the details. Also check out the Scraperwiki documentation: https://scraperwiki.com/docs/python/

When should I make the investment to learn how to scrape?
A few reasons (non-exhaustive list!):

1. If you regularly have to extract data where there are numerous tables in one page.
2. If your information is spread across numerous pages.
4. If you want to run the scraper regularly (e.g. if information is released every week or month).
If you want things like email alerts if information on a particular webpage changes.
…And you don’t want to pay someone else to do it for you!

Summary:
In this course we’ve covered Web scraping and how to extract data from websites. The main function of scraping is to convert data that is semi-structured into structured data and make it easily useable for further processing. While this is a relatively simple task with a bit of programming – for single webpages it is also feasible without any programming at all. We’ve introduced =importHTML and the Scraper extension for your scraping needs.

Further Reading
Scraping for Journalism: A Guide for Collecting Data: ProPublica Guides
Scraping for Journalists (ebook): Paul Bradshaw
Scrape the Web: Strategies for programming websites that don’t expect it : Talk from PyCon
An Introduction to Compassionate Screen Scraping: Will Larson
- See more at: http://schoolofdata.org/handbook/courses/scraping/#sthash.5Pok0BWj.dpuf
